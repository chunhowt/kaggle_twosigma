{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import Imputer, MaxAbsScaler, RobustScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(actual, pred):\n",
    "    \"\"\"Computes the R value of the predictions given the actual values.\n",
    "    \n",
    "    Both actual and pred should be of the same length where each index\n",
    "    corresponds to the actual and predicted value of the same row of data.\n",
    "    \n",
    "    Args:\n",
    "        actual: A list of actual values.\n",
    "        pred: A list of predicted values.\n",
    "    \n",
    "    Returns:\n",
    "        R value of the predictions given the actual values.\n",
    "    \"\"\"\n",
    "    assert len(actual) == len(pred)\n",
    "    nume = np.sum(np.square(actual - pred))\n",
    "    denom = np.sum(np.square(actual - np.mean(actual)))\n",
    "    r_sq = 1 - nume / denom\n",
    "    return np.sign(r_sq) * np.sqrt(np.abs(r_sq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"A generic Model interface to encapsulate modeling and data processing.\n",
    "    \n",
    "    Attributes:\n",
    "        model_: A sklearn model with two function interfaces - fit, predict.\n",
    "        imputer_: A sklearn preprocessing model to perform imputation for missing values.\n",
    "        normalizer_: A sklearn preprocessing model to normalize feature values.\n",
    "        columns_: List of column names corresponding to the features.\n",
    "    \"\"\"\n",
    "    def __init__(self, model=None, imputer=None, normalizer=None):\n",
    "        \"\"\"Initialize the Model class with the given model.\n",
    "        \n",
    "        Args:\n",
    "            model: A sklearn model.\n",
    "            imputer: Transformer for missing values.\n",
    "            normalizer: Normalizer for the feature values.\n",
    "        \"\"\"\n",
    "        self.model_ = model\n",
    "        self.imputer_ = imputer\n",
    "        self.normalizer_ = normalizer\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"Fit the underlying model with the given data frame.\n",
    "        \n",
    "        Args:\n",
    "            df: Dataframe object containing the underlying data. It has three\n",
    "                special columns - id, y (label), timestamp.\n",
    "        \"\"\"\n",
    "        # Drops the three special columns.\n",
    "        X_train = df.drop(['id', 'y', 'timestamp'], axis=1)\n",
    "        self.columns_ = X_train.columns\n",
    "        \n",
    "        # Imputes the missing value.\n",
    "        if self.imputer_:\n",
    "            X_train = self.imputer_.fit_transform(X_train)\n",
    "        else:\n",
    "            # Fill with 0 if there is no imputer.\n",
    "            X_train = X_train.fillna(0)\n",
    "        \n",
    "        # Normalizes the feature values.\n",
    "        if self.normalizer_:\n",
    "            X_train = self.normalizer_.fit_transform(X_train)\n",
    "        \n",
    "        Y_train = df['y']\n",
    "        self.model_.fit(X_train, Y_train)\n",
    "    \n",
    "    def predict(self, df):\n",
    "        \"\"\"Infers the predictions for the given df features.\n",
    "        \n",
    "        Args:\n",
    "            df: Dataframe object containing the data features to be predicted. It has\n",
    "                two special columns - id, timestamp.\n",
    "        \n",
    "        Returns:\n",
    "            The predictions for each of the row in df, in the same order as given.\n",
    "        \"\"\"\n",
    "        X_pred = df.drop(['id', 'timestamp'], axis=1)\n",
    "        \n",
    "        # Imputes the missing value.\n",
    "        if self.imputer_:\n",
    "            X_pred = self.imputer_.transform(X_pred)\n",
    "        else:\n",
    "            # Fill with 0 if there is no imputer.\n",
    "            X_pred = X_pred.fillna(0)\n",
    "        \n",
    "        # Normalizes the feature values.\n",
    "        if self.normalizer_:\n",
    "            X_pred = self.normalizer_.transform(X_pred)\n",
    "        \n",
    "        return self.model_.predict(X_pred)\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\"Visualize the underlying learned model.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearRegressionModel(Model):\n",
    "    def visualize(self):\n",
    "        assert len(self.columns_) == len(self.model_.coef_), \\\n",
    "            \"# columns: %d vs # coeff: %d\" % (len(self.columns_), len(self.model_.coef_))\n",
    "        print('Bias: ', self.model_.intercept_)\n",
    "        for (ind, column) in enumerate(self.columns_):\n",
    "            print(column, ': ', self.model_.coef_[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_df):\n",
    "    \"\"\"Returns a new model trained using the given data frame.\n",
    "\n",
    "    Args:\n",
    "        train_df: DataFrame object corresponding to training data.\n",
    "    \n",
    "    Returns:\n",
    "        Model trained using the data frame.\n",
    "    \"\"\"\n",
    "    model = LinearRegressionModel(\n",
    "        model=LinearRegression(),\n",
    "        imputer=Imputer(strategy='mean'),\n",
    "        normalizer=RobustScaler())\n",
    "    model.fit(train_df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_infer(model, features, target):\n",
    "    \"\"\"Populates the target using the features and model and updates the\n",
    "    model.\n",
    "    \n",
    "    Args:\n",
    "        model: A model for prediction and then for updating with new data.\n",
    "        features: DataFrame object corresponding to new data to predict\n",
    "                  on, and then update the existing model.\n",
    "        target: DataFrame object containing id and the y value to be updated\n",
    "                with.\n",
    "    \"\"\"\n",
    "    # Verify that the features and targets are aligned.\n",
    "    assert len(features) == len(target)\n",
    "    assert (features['id'] == target['id']).all()\n",
    "\n",
    "    # Set the target value to the one given.\n",
    "    target['y'] = model.predict(features)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias:  -3.84225154579e-05\n",
      "derived_0 :  0.0151059797238\n",
      "derived_1 :  -0.00395201371901\n",
      "derived_2 :  0.00538675428021\n",
      "derived_3 :  0.00514174909599\n",
      "derived_4 :  0.0161932535441\n",
      "fundamental_0 :  0.000501628003804\n",
      "fundamental_1 :  18126.3066486\n",
      "fundamental_2 :  -0.101077739328\n",
      "fundamental_3 :  -0.000103815948183\n",
      "fundamental_5 :  0.00736938062179\n",
      "fundamental_6 :  0.000798820447922\n",
      "fundamental_7 :  0.0115993382782\n",
      "fundamental_8 :  0.00128767567396\n",
      "fundamental_9 :  0.000604576797994\n",
      "fundamental_10 :  0.00331272159282\n",
      "fundamental_11 :  0.00480294412597\n",
      "fundamental_12 :  0.00155168714328\n",
      "fundamental_13 :  0.000965376688291\n",
      "fundamental_14 :  -0.0311213536972\n",
      "fundamental_15 :  -0.0335742640051\n",
      "fundamental_16 :  0.0180404644843\n",
      "fundamental_17 :  0.00529361001883\n",
      "fundamental_18 :  -0.0235061447511\n",
      "fundamental_19 :  0.000579897467811\n",
      "fundamental_20 :  116.678032663\n",
      "fundamental_21 :  0.000877126584698\n",
      "fundamental_22 :  0.0477748186829\n",
      "fundamental_23 :  -0.0543540821123\n",
      "fundamental_24 :  2.87277117943e-05\n",
      "fundamental_25 :  0.000250386215795\n",
      "fundamental_26 :  0.0018751455796\n",
      "fundamental_27 :  0.000901793420479\n",
      "fundamental_28 :  0.000746565325306\n",
      "fundamental_29 :  -0.00122145236607\n",
      "fundamental_30 :  -0.00351955420845\n",
      "fundamental_31 :  8.86020610835e-06\n",
      "fundamental_32 :  0.00167926559205\n",
      "fundamental_33 :  -0.000511571095972\n",
      "fundamental_34 :  -2.33522801302e-05\n",
      "fundamental_35 :  0.0004890456852\n",
      "fundamental_36 :  -0.0121051159909\n",
      "fundamental_37 :  0.00663122527425\n",
      "fundamental_38 :  1.29312031731e-05\n",
      "fundamental_39 :  -0.000629213099135\n",
      "fundamental_40 :  4.87361508021e-05\n",
      "fundamental_41 :  0.0283633563659\n",
      "fundamental_42 :  -0.0271303344732\n",
      "fundamental_43 :  -0.000644716426223\n",
      "fundamental_44 :  0.0064837118913\n",
      "fundamental_45 :  116.676983917\n",
      "fundamental_46 :  -7.09752487182e-06\n",
      "fundamental_47 :  -0.000309684818035\n",
      "fundamental_48 :  0.0012071452359\n",
      "fundamental_49 :  -0.000346281588218\n",
      "fundamental_50 :  0.0538140662096\n",
      "fundamental_51 :  0.0820279227831\n",
      "fundamental_52 :  -0.00213291558823\n",
      "fundamental_53 :  0.0157136330733\n",
      "fundamental_54 :  -0.000128101487576\n",
      "fundamental_55 :  0.0127417938394\n",
      "fundamental_56 :  0.0107305231936\n",
      "fundamental_57 :  0.000183393127177\n",
      "fundamental_58 :  5.44378719951e-05\n",
      "fundamental_59 :  0.000148085869569\n",
      "fundamental_60 :  0.00678949703862\n",
      "fundamental_61 :  18126.3061504\n",
      "fundamental_62 :  -0.000113103134609\n",
      "fundamental_63 :  0.000197880193156\n",
      "technical_0 :  -4.03377239309e-05\n",
      "technical_1 :  0.000101994659531\n",
      "technical_2 :  -6.2046758103e-05\n",
      "technical_3 :  8.82083895704e-05\n",
      "technical_5 :  -0.000126830913359\n",
      "technical_6 :  7.50745168716e-05\n",
      "technical_7 :  -0.000482077470963\n",
      "technical_9 :  -0.0005206324185\n",
      "technical_10 :  -4.06480921811e-06\n",
      "technical_11 :  -0.000117671173484\n",
      "technical_12 :  -3.79202729164e-05\n",
      "technical_13 :  -0.000234199288982\n",
      "technical_14 :  0.000163371129617\n",
      "technical_16 :  0.000432160463333\n",
      "technical_17 :  -0.000165854368788\n",
      "technical_18 :  -0.00020789600228\n",
      "technical_19 :  -0.0039422705147\n",
      "technical_20 :  -0.00685008068984\n",
      "technical_21 :  0.00164221032162\n",
      "technical_22 :  8.35759053643e-05\n",
      "technical_24 :  6.70320264362e-06\n",
      "technical_25 :  -9.74609587132e-05\n",
      "technical_27 :  0.00195940701451\n",
      "technical_28 :  -0.000125960461219\n",
      "technical_29 :  -0.000132612711241\n",
      "technical_30 :  0.00934237038391\n",
      "technical_31 :  -0.000163648115176\n",
      "technical_32 :  -0.000217656179302\n",
      "technical_33 :  5.83443994628e-06\n",
      "technical_34 :  -9.26149532461e-05\n",
      "technical_35 :  -0.00117589464602\n",
      "technical_36 :  -0.00059803142716\n",
      "technical_37 :  9.4546523087e-05\n",
      "technical_38 :  -5.49772960312e-05\n",
      "technical_39 :  -0.000164539348589\n",
      "technical_40 :  -0.00104526562177\n",
      "technical_41 :  -8.00191162398e-05\n",
      "technical_42 :  -0.000310329876356\n",
      "technical_43 :  -0.000127371766439\n",
      "technical_44 :  4.16716034124e-05\n",
      "-0.345930004229\n"
     ]
    }
   ],
   "source": [
    "# A cell to perform local E2E training and prediction that simulates\n",
    "# the KaggleGym behavior.\n",
    "def localE2E(run_validation=True, visualize_model=False):\n",
    "    \"\"\"A function to simulate KaggleGym behavior of predicting per timestep.\n",
    "    \"\"\"\n",
    "    # Here's an example of loading the CSV using Pandas's built-in HDF5 support:\n",
    "    with pd.HDFStore(\"train.h5\", \"r\") as train:\n",
    "        # Note that the \"train\" dataframe is the only dataframe in the file\n",
    "        df = train.get(\"train\")\n",
    "        train_df = df[:806298]\n",
    "        valid_df = df[806298:]\n",
    "    \n",
    "    # NOTE: Training goes here.\n",
    "    model = train_model(train_df)\n",
    "    if visualize_model:\n",
    "        model.visualize()\n",
    "    \n",
    "    if not run_validation:\n",
    "        return 0\n",
    "    \n",
    "    # Validation goes here.\n",
    "    predictions = []\n",
    "    curr_timestamp = 0\n",
    "    curr_data = []\n",
    "    curr_target = []\n",
    "    for row in valid_df.itertuples():\n",
    "        # If it is a new timestamp, predict.\n",
    "        if row.timestamp != curr_timestamp:\n",
    "            if curr_data:\n",
    "                # NOTE: Inference and updating goes here.\n",
    "                new_targets = run_infer(\n",
    "                    model,\n",
    "                    # Drop the last column (y).\n",
    "                    pd.DataFrame(curr_data, columns=valid_df.columns[:-1]),\n",
    "                    pd.DataFrame(curr_target, columns=['id', 'y']))\n",
    "                predictions.extend(list(new_targets['y']))\n",
    "            # Reset arrays.\n",
    "            curr_data = []\n",
    "            curr_target= []\n",
    "            curr_timestamp = row.timestamp\n",
    "        # Drop the first (index) and last (y) into features.\n",
    "        curr_data.append(list(row)[1:-1])\n",
    "        curr_target.append([row.id, 0])\n",
    "\n",
    "    # Infer for the last timestamp.\n",
    "    new_targets = run_infer(\n",
    "        model,\n",
    "        # Drop the last column (y).\n",
    "        pd.DataFrame(curr_data, columns=valid_df.columns[:-1]),\n",
    "        pd.DataFrame(curr_target, columns=['id', 'y']))\n",
    "\n",
    "    predictions.extend(list(new_targets['y']))\n",
    "    return evaluate(valid_df['y'], predictions)\n",
    "\n",
    "print(localE2E(run_validation=True, visualize_model=True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearRegression Results\n",
    "#### In general, LinearRegression does not perform very well, with only about -0.002833 for the best result\n",
    "\n",
    "<pre>\n",
    "\n",
    "|R value   | Model            | Normalization | Features | Use Id | Online | Missing |\n",
    "|----------|------------------|---------------|----------|--------|--------|---------|\n",
    "|-0.008351 | Always set to 0  |               |          |        |        |         |\n",
    "|-0.005666 | Linear regression| None          | Original |   No   |   No   |    0    |\n",
    "|-0.074787 | Linear regression| None          | Original |   No   |   No   |  mean   |\n",
    "|-0.060731 | Linear regression| None          | Original |   No   |   No   | median  |\n",
    "|-0.293456 | Linear regression| normalize=true| Original |   No   |   No   |    0    |\n",
    "|-0.329960 | Linear regression| normalize=true| Original |   No   |   No   |  mean   |\n",
    "|-0.002833 | Linear regression| RS            | Original |   No   |   No   |    0    |\n",
    "|-0.005737 | Linear regression| RS            | Original |   No   |   No   |  mean   |\n",
    "|-0.002833 | Linear regression| RS            | Original |   No   |   No   | median  |\n",
    "|-0.345930 | Linear regression| SS            | Original |   No   |   No   |    0    |\n",
    "|-0.329960 | Linear regression| SS            | Original |   No   |   No   |  mean   |\n",
    "|-0.348907 | Linear regression| SS            | Original |   No   |   No   | median  |\n",
    "|-0.345930 | Linear regression| MAS           | Original |   No   |   No   |    0    |\n",
    "|-0.329960 | Linear regression| MAS           | Original |   No   |   No   |  mean   |\n",
    "|-0.348907 | Linear regression| MAS           | Original |   No   |   No   | median  |\n",
    "\n",
    "</pre>\n",
    "\n",
    "Column descriptions:\n",
    "- Normalization: Whether we perform normalization on the feature values.\n",
    "  - RS: RobustScaler\n",
    "  - SS: StandardScaler\n",
    "  - MAS: MaxAbsScaler\n",
    "- Features: The features used by model.\n",
    "- Id: Whether we use the id to train a different model per row.\n",
    "- Online: Whether we use the eval data to update the model param.\n",
    "- Missing: What value do we replace with the missing value.\n",
    "\n",
    "## Details\n",
    "\n",
    "For Linear regression without any normalization, the coefficient for each feature is very low (around e^-20 - e^-30), probably because the features are not normalized and thus too high. It also seems like one of the weight (derived_3) is 0. Hypothesis is that derived_features is computed from fundamental features.\n",
    "\n",
    "Trying a bunch of imputation for NaN and normalization techniques.\n",
    "\n",
    "Normalization:\n",
    "- Not doing any normalization actually perform relatively well, especially since it is a linear regression. Only RobustScaler performs better.\n",
    "- normalize=True for LinearRegression model seems to perform a lot worse than not doing normalization (most likely it is a StandardScaler)\n",
    "- RobustScaler performs relatively well.\n",
    "- StandardScaler and MaxAbsScaler performs badly.\n",
    "\n",
    "Imputation:\n",
    "- Missing value = 0 is actually fairly good.\n",
    "- Missing value = \"most frequent\" needs a really long time for training, not sure why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data from Kagglegym\n",
    "import kagglegym\n",
    "\n",
    "def kaggleE2E():\n",
    "    \"\"\"Runs Kaggle training and predictions E2E.\n",
    "    \"\"\"\n",
    "    # Create env.\n",
    "    env = kagglegym.make()\n",
    "    # Get first observation.\n",
    "    observation = env.reset()\n",
    "    # Get train data.\n",
    "    train_df = observation.train\n",
    "    model = train_model(train_df)\n",
    "\n",
    "    # Predict using Kagglegym.\n",
    "    while True:\n",
    "        predictions = run_infer(model, observation.features, observation.target)\n",
    "        observation, reward, done, info = env.step(predictions)\n",
    "        # print('Reward: ', reward)\n",
    "        if done:\n",
    "            break\n",
    "    print(info)\n",
    "\n",
    "kaggleE2E()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
