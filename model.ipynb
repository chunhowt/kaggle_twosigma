{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor, HuberRegressor\n",
    "from sklearn.preprocessing import Imputer, RobustScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(actual, pred):\n",
    "    \"\"\"Computes the R value of the predictions given the actual values.\n",
    "    \n",
    "    Both actual and pred should be of the same length where each index\n",
    "    corresponds to the actual and predicted value of the same row of data.\n",
    "    \n",
    "    Args:\n",
    "        actual: A list of actual values.\n",
    "        pred: A list of predicted values.\n",
    "    \n",
    "    Returns:\n",
    "        R value of the predictions given the actual values.\n",
    "    \"\"\"\n",
    "    assert len(actual) == len(pred)\n",
    "    nume = np.sum(np.square(actual - pred))\n",
    "    denom = np.sum(np.square(actual - np.mean(actual)))\n",
    "    r_sq = 1 - nume / denom\n",
    "    r_score =  np.sign(r_sq) * np.sqrt(np.abs(r_sq))\n",
    "\n",
    "    print('======================')\n",
    "    print('Evaluation results....')\n",
    "    print('Min pred: ', min(pred), ', Max pred: ', max(pred))\n",
    "    for percent in range(10, 100, 10):\n",
    "        print('Percentile ', percent, ': ', np.percentile(pred, percent))\n",
    "        \n",
    "    print('Min actual: ', min(actual), ', Max actual: ', max(actual))\n",
    "    for percent in range(10, 100, 10):\n",
    "        print('Percentile ', percent, ': ', np.percentile(actual, percent))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    pd.Series(pred).hist(bins=100, range=(np.percentile(pred, 5), np.percentile(pred, 95)), ax=ax1)\n",
    "    ax1.set_title('Predicted values')\n",
    "    pd.Series(actual).hist(bins=100, range=(np.percentile(actual, 5), np.percentile(actual, 95)), ax=ax2)\n",
    "    ax2.set_title('Actual values')\n",
    "    plt.show()\n",
    "    \n",
    "    return r_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"A generic Model interface to encapsulate modeling and data processing.\n",
    "    \n",
    "    Attributes:\n",
    "        model_: A sklearn model with two function interfaces - fit, predict.\n",
    "        imputer_: A sklearn preprocessing model to perform imputation for missing values.\n",
    "        normalizer_: A sklearn preprocessing model to normalize feature values.\n",
    "        columns_: List of column names corresponding to the features.\n",
    "    \"\"\"\n",
    "    def __init__(self, model=None, imputer=None, normalizer=None):\n",
    "        \"\"\"Initialize the Model class with the given model.\n",
    "        \n",
    "        Args:\n",
    "            model: A sklearn model.\n",
    "            imputer: Transformer for missing values.\n",
    "            normalizer: Normalizer for the feature values.\n",
    "        \"\"\"\n",
    "        self.model_ = model\n",
    "        self.imputer_ = imputer\n",
    "        self.normalizer_ = normalizer\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"Fit the underlying model with the given data frame.\n",
    "        \n",
    "        Args:\n",
    "            df: Dataframe object containing the underlying data. It has three\n",
    "                special columns - id, y (label), timestamp.\n",
    "        \"\"\"\n",
    "        # Drops the three special columns.\n",
    "        X_train = df.drop(['id', 'y', 'timestamp'], axis=1)\n",
    "        X_train = X_train.filter(regex='technical_*', axis=1)\n",
    "        self.columns_ = X_train.columns\n",
    "        \n",
    "        # Imputes the missing value.\n",
    "        if self.imputer_:\n",
    "            X_train = self.imputer_.fit_transform(X_train)\n",
    "        else:\n",
    "            # Fill with 0 if there is no imputer.\n",
    "            X_train = X_train.fillna(0)\n",
    "        \n",
    "        # Normalizes the feature values.\n",
    "        if self.normalizer_:\n",
    "            X_train = self.normalizer_.fit_transform(X_train)\n",
    "        \n",
    "        Y_train = df['y']\n",
    "        self.model_.fit(X_train, Y_train)\n",
    "        self.min_y_ = min(df['y'])\n",
    "        self.max_y_ = max(df['y'])\n",
    "    \n",
    "    def predict(self, df):\n",
    "        \"\"\"Infers the predictions for the given df features.\n",
    "        \n",
    "        Args:\n",
    "            df: Dataframe object containing the data features to be predicted. It has\n",
    "                two special columns - id, timestamp.\n",
    "        \n",
    "        Returns:\n",
    "            The predictions for each of the row in df, in the same order as given.\n",
    "        \"\"\"\n",
    "        X_pred = df.drop(['id', 'timestamp'], axis=1)\n",
    "        X_pred = X_pred.filter(regex='technical_*', axis=1)\n",
    "        \n",
    "        # Imputes the missing value.\n",
    "        if self.imputer_:\n",
    "            X_pred = self.imputer_.transform(X_pred)\n",
    "        else:\n",
    "            # Fill with 0 if there is no imputer.\n",
    "            X_pred = X_pred.fillna(0)\n",
    "        \n",
    "        # Normalizes the feature values.\n",
    "        if self.normalizer_:\n",
    "            X_pred = self.normalizer_.transform(X_pred)\n",
    "        \n",
    "        # Hack: Threshold each y between (self.min_y_, self.max_y_) to avoid outlier features from killing\n",
    "        # the predictions.\n",
    "        return list(map(\n",
    "            lambda y: self.min_y_ if y < self.min_y_ else min(y, self.max_y_),\n",
    "            self.model_.predict(X_pred)))\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\"Visualize the underlying learned model.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearModel(Model):\n",
    "    \"\"\"Wrapper around sklearn.linear_model.\n",
    "    \"\"\"\n",
    "    def visualize(self):\n",
    "        try:\n",
    "            assert len(self.columns_) == len(self.model_.coef_), \\\n",
    "            \"# columns: %d vs # coeff: %d\" % (len(self.columns_), len(self.model_.coef_))\n",
    "            print('Bias: ', self.model_.intercept_)\n",
    "            for (ind, column) in enumerate(self.columns_):\n",
    "                print(column, ': ', self.model_.coef_[ind])\n",
    "        except:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnsembleModel(Model):\n",
    "    \"\"\"Wrapper around sklearn.ensemble.\n",
    "    \"\"\"\n",
    "    def visualize(self):\n",
    "        try:\n",
    "            assert len(self.columns_) == len(self.model_.feature_importances_), \\\n",
    "            \"# columns: %d vs # features: %d\" % (len(self.columns_), len(self.model_.feature_importances_))\n",
    "            for (ind, column) in enumerate(self.columns_):\n",
    "                print(column, ': ', self.model_.feature_importances_[ind])\n",
    "        except:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_df):\n",
    "    \"\"\"Returns a new model trained using the given data frame.\n",
    "\n",
    "    Args:\n",
    "        train_df: DataFrame object corresponding to training data.\n",
    "    \n",
    "    Returns:\n",
    "        Model trained using the data frame.\n",
    "    \"\"\"\n",
    "    model = LinearModel(\n",
    "        model=LinearRegression(),\n",
    "        imputer=Imputer(strategy='median'),\n",
    "        normalizer=StandardScaler())\n",
    "    model.fit(train_df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_infer(model, features, target):\n",
    "    \"\"\"Populates the target using the features and model and updates the\n",
    "    model.\n",
    "    \n",
    "    Args:\n",
    "        model: A model for prediction and then for updating with new data.\n",
    "        features: DataFrame object corresponding to new data to predict\n",
    "                  on, and then update the existing model.\n",
    "        target: DataFrame object containing id and the y value to be updated\n",
    "                with.\n",
    "    \"\"\"\n",
    "    # Verify that the features and targets are aligned.\n",
    "    assert len(features) == len(target)\n",
    "    assert (features['id'] == target['id']).all()\n",
    "\n",
    "    # Set the target value to the one given.\n",
    "    target['y'] = model.predict(features)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A cell to perform local E2E training and prediction that simulates\n",
    "# the KaggleGym behavior.\n",
    "def localE2E(run_validation=True, visualize_model=False):\n",
    "    \"\"\"A function to simulate KaggleGym behavior of predicting per timestep.\n",
    "    \"\"\"\n",
    "    # Here's an example of loading the CSV using Pandas's built-in HDF5 support:\n",
    "    with pd.HDFStore(\"train.h5\", \"r\") as train:\n",
    "        # Note that the \"train\" dataframe is the only dataframe in the file\n",
    "        df = train.get(\"train\")\n",
    "        train_df = df[:806298]\n",
    "        valid_df = df[806298:]\n",
    "    \n",
    "    # NOTE: Training goes here.\n",
    "    model = train_model(train_df)\n",
    "    if visualize_model:\n",
    "        model.visualize()\n",
    "\n",
    "    train_preds = run_infer(\n",
    "        model,\n",
    "        train_df.drop(['y'], axis=1),\n",
    "        train_df[['id', 'y']])\n",
    "    print('Training R score: ', evaluate(train_df['y'], train_preds['y']))\n",
    "    \n",
    "    if not run_validation:\n",
    "        return 0\n",
    "    \n",
    "    # Validation goes here.\n",
    "    predictions = []\n",
    "    curr_timestamp = 0\n",
    "    curr_data = []\n",
    "    curr_target = []\n",
    "    for row in valid_df.itertuples():\n",
    "        # If it is a new timestamp, predict.\n",
    "        if row.timestamp != curr_timestamp:\n",
    "            if curr_data:\n",
    "                # NOTE: Inference and updating goes here.\n",
    "                new_targets = run_infer(\n",
    "                    model,\n",
    "                    # Drop the last column (y).\n",
    "                    pd.DataFrame(curr_data, columns=valid_df.columns[:-1]),\n",
    "                    pd.DataFrame(curr_target, columns=['id', 'y']))\n",
    "                predictions.extend(list(new_targets['y']))\n",
    "            # Reset arrays.\n",
    "            curr_data = []\n",
    "            curr_target= []\n",
    "            curr_timestamp = row.timestamp\n",
    "        # Drop the first (index) and last (y) into features.\n",
    "        curr_data.append(list(row)[1:-1])\n",
    "        curr_target.append([row.id, 0])\n",
    "\n",
    "    # Infer for the last timestamp.\n",
    "    new_targets = run_infer(\n",
    "        model,\n",
    "        # Drop the last column (y).\n",
    "        pd.DataFrame(curr_data, columns=valid_df.columns[:-1]),\n",
    "        pd.DataFrame(curr_target, columns=['id', 'y']))\n",
    "\n",
    "    predictions.extend(list(new_targets['y']))\n",
    "    return evaluate(valid_df['y'], predictions)\n",
    "\n",
    "print('Validation R score: ', localE2E(run_validation=True, visualize_model=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data from Kagglegym\n",
    "import kagglegym\n",
    "\n",
    "def kaggleE2E():\n",
    "    \"\"\"Runs Kaggle training and predictions E2E.\n",
    "    \"\"\"\n",
    "    # Create env.\n",
    "    env = kagglegym.make()\n",
    "    # Get first observation.\n",
    "    observation = env.reset()\n",
    "    # Get train data.\n",
    "    train_df = observation.train\n",
    "    model = train_model(train_df)\n",
    "\n",
    "    # Predict using Kagglegym.\n",
    "    while True:\n",
    "        predictions = run_infer(model, observation.features, observation.target)\n",
    "        observation, reward, done, info = env.step(predictions)\n",
    "        # print('Reward: ', reward)\n",
    "        if done:\n",
    "            break\n",
    "    print(info)\n",
    "\n",
    "kaggleE2E()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
